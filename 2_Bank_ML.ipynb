{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a66093",
   "metadata": {},
   "source": [
    "# 2. Bank - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fef8c",
   "metadata": {},
   "source": [
    "We have done the Exploratory Data Analysis in the [Last Notebook](1_Bank_EDA.ipynb), now it's time to actually start the Machine Learning Part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d18fcc",
   "metadata": {},
   "source": [
    "## 2.1 Base Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8a0f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=r\".*A worker stopped while some jobs were given to the executor.*\",\n",
    "    category=UserWarning, module=r\"joblib[.]externals[.]loky[.]process_executor\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Using `tqdm.autonotebook.tqdm` in notebook mode.*\",\n",
    "    module=r\"tqdm_joblib\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn import set_config\n",
    "from bank_functions import *\n",
    "from bank_feat_engineer import *\n",
    "from bank_models import *\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "N_FOLDS = 3\n",
    "\n",
    "result_folder = \"Results\"\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "set_config(transform_output=\"pandas\")   \n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(r\"Data/train.csv\")\n",
    "df_test = pd.read_csv(r\"Data/test.csv\") \n",
    "\n",
    "target = \"y\"\n",
    "X_cat = [\"job\",\"month\",\"poutcome\",\"education\",\"contact\",\"marital\",\"loan\",\"housing\",\"default\"]\n",
    "X_num = [\"balance\",\"duration\",\"pdays\",\"age\",\"campaign\",\"previous\",\"day\"]\n",
    "\n",
    "# Set categorical columns to 'category' dtype\n",
    "for col in X_cat:\n",
    "    df_train[col] = df_train[col].astype('category')\n",
    "    df_test[col]  = df_test[col].astype('category')\n",
    "    \n",
    "X = df_train[X_cat + X_num]\n",
    "y = df_train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab6b45",
   "metadata": {},
   "source": [
    "The goal is to maximize the AUC of the ROC Curve. We are going to test different algorithms from multiple families. We set different preprocessors on the different algorithms as some of them need imputing missing values, encoding categorical variables or scaling numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb117832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = {\n",
    "    \"base\": make_preprocessor(X_num, X_cat),\n",
    "    \"imp\": make_preprocessor(X_num, X_cat, impute=True),\n",
    "    \"imp_onehot\": make_preprocessor(X_num, X_cat, onehot=True, sparse_output=True, scaler=True),\n",
    "    \"imp_dense\": make_preprocessor(X_num, X_cat, onehot=True, sparse_output=False),\n",
    "    \"dense\": make_preprocessor(X_num, X_cat, onehot=True, sparse_output=False),\n",
    "    \"imp_dense_sc\": make_preprocessor(X_num, X_cat, onehot=True, sparse_output=False, scaler=True),\n",
    "    \"imp_ordinal\": make_preprocessor(X_num, X_cat, ordinal=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34442626",
   "metadata": {},
   "source": [
    "We are going to use a diverse set of estimators to capture the maximum variance possible. Each pipeline is tuned with cross-validation and model-appropriate search strategies:\n",
    "- Fast and Strong baselines Gradient Boosting Models: **LightGBM, XGBoost, CatBoost and HistGradientBoosting**\n",
    "- Bagging / Randomized trees: **Random Forest and ExtraTrees**\n",
    "- Purely Additive Boosting : **Explainable Boosting Machine**\n",
    "- Linear and Distance/Margin models : **ElasticNet, k NN and SVC**\n",
    "- Neural-based tabular models : **Shallow MLP and TabNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10aa273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lgbm\n",
      "Loading xgb\n",
      "Loading cat\n",
      "Loading hgb\n",
      "Loading rf\n",
      "Loading et\n",
      "Loading ebm\n",
      "Loading lr\n",
      "Loading knn\n",
      "Loading svc\n",
      "Loading nn\n",
      "Loading tab\n"
     ]
    }
   ],
   "source": [
    "# Unified configuration list \n",
    "configs = [\n",
    "    (\"lgbm\",run_lightGBM,      {\"preprocessor\": pre[\"base\"], \"n_jobs\": 16}),\n",
    "    (\"xgb\", run_XGBoost,       {\"preprocessor\": pre[\"base\"]}),\n",
    "    (\"cat\", run_CatBoost,      {\"preprocessor\": pre[\"base\"], \"X_cat\": X_cat, \"n_jobs\": 2}),\n",
    "    (\"hgb\", run_HGB,           {\"preprocessor\": pre[\"imp_dense\"]}),\n",
    "    (\"rf\",  run_randomForest,  {\"preprocessor\": pre[\"dense\"]}),\n",
    "    (\"et\",  run_extraTrees,    {\"preprocessor\": pre[\"imp_dense\"]}),\n",
    "    (\"ebm\", run_EBM,           {\"preprocessor\": pre[\"imp\"]}),\n",
    "    (\"lr\",  run_LogReg,        {\"preprocessor\": pre[\"imp_onehot\"]}),\n",
    "    (\"knn\", run_KNN,           {\"preprocessor\": pre[\"imp_dense_sc\"], \"n_jobs\": 4}),\n",
    "    (\"svc\", run_SVC,           {\"preprocessor\": pre[\"imp_dense_sc\"], \"n_jobs\": 16}),\n",
    "    (\"nn\",  run_NeuralNetwork, {\"preprocessor\": pre[\"imp_dense_sc\"]}),\n",
    "    (\"tab\", run_tabnet,        {\"X_num\": X_num, \"X_cat\": X_cat, \"preprocessor\": pre[\"imp_ordinal\"], \"n_candidates\": 20}),\n",
    "]\n",
    "\n",
    "# Run models or load them from disk \n",
    "model_folder = os.path.join(result_folder, \"models\")\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "overwrite_model = False\n",
    "models = {}\n",
    "\n",
    "for key, func, kw in configs:\n",
    "    \n",
    "    model_path = os.path.join(model_folder, f\"models_{key}.joblib\")\n",
    "    if overwrite_model or not os.path.exists(model_path):\n",
    "        print(f\"Running {key}\")\n",
    "        search = func(X=X, y=y, **kw)\n",
    "        joblib.dump(search, model_path, compress=3)\n",
    "        models[key] = search\n",
    "    else:\n",
    "        print(f\"Loading {key}\")\n",
    "        models[key] = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Predictions to submit to Kaggle\n",
    "pred_folder = os.path.join(result_folder, \"predictions\")\n",
    "os.makedirs(pred_folder, exist_ok=True)\n",
    "overwrite_pred = False\n",
    "\n",
    "for model_name, search in models.items():\n",
    "    pred_path = os.path.join(pred_folder, f\"predictions_{model_name}.csv\")\n",
    "    if overwrite_pred or not os.path.exists(pred_path):\n",
    "        print(f\"Predicting {model_name}\")\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        write_predictions(\n",
    "            df=df_test,\n",
    "            model=search.best_estimator_,\n",
    "            features=X_cat + X_num,\n",
    "            target=target,\n",
    "            path= pred_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95eed2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm OOF Loaded\n",
      "xgb OOF Loaded\n",
      "cat OOF Loaded\n",
      "hgb OOF Loaded\n",
      "rf OOF Loaded\n",
      "et OOF Loaded\n",
      "ebm OOF Loaded\n",
      "lr OOF Loaded\n",
      "knn OOF Loaded\n",
      "svc OOF Loaded\n",
      "nn OOF Loaded\n",
      "tab OOF Computed\n"
     ]
    }
   ],
   "source": [
    "# Compute cross-validation OOF predictions for analysis in the final notebook\n",
    "cv_folder = os.path.join(result_folder, \"cross_validation\")\n",
    "os.makedirs(cv_folder, exist_ok=True)\n",
    "\n",
    "overwrite_cv = False\n",
    "cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof_list = []\n",
    "\n",
    "for model_name, search in models.items():\n",
    "    scores_path = os.path.join(cv_folder, f\"cv_{model_name}_scores.npy\")\n",
    "    best = search.best_estimator_\n",
    "\n",
    "    if overwrite_cv or not os.path.exists(scores_path):\n",
    "        y_score = cross_val_predict(best, X, y, cv=cv, n_jobs=3, method=\"predict_proba\")[:, 1]\n",
    "        np.save(scores_path, y_score)\n",
    "        print(f\"{model_name} OOF Computed\")\n",
    "    else:\n",
    "        y_score = np.load(scores_path, allow_pickle=True)\n",
    "        print(f\"{model_name} OOF Loaded\")\n",
    "\n",
    "    oof_list.append(y_score)\n",
    "\n",
    "df_oof = pd.DataFrame(np.vstack(oof_list).T, columns=list(models.keys()))\n",
    "# df_oof.to_csv(os.path.join(result_folder, \"cv_oof.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5c50a",
   "metadata": {},
   "source": [
    "## 2.2 Stacking Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cc684",
   "metadata": {},
   "source": [
    "After identifying strong individual learners, we build a **StackingClassifier** where the final estimator is LightGBM as it allows the ensemble to:\n",
    "- Weight the different mistakes of the single models\n",
    "- Blend high-bias and high-variance learners to optimize AUC without over-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c81b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_stack = True\n",
    "stack_path = os.path.join(model_folder, \"models_stack.joblib\")\n",
    "\n",
    "if overwrite_stack or not os.path.exists(stack_path):\n",
    "    stack = StackingClassifier(\n",
    "        estimators=[(model_name, search.best_estimator_) for model_name, search in models.items()],\n",
    "        final_estimator= lgb_default(seed=SEED, n_estimator = 2500),\n",
    "        cv=cv,\n",
    "        n_jobs=1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    with tqdm_joblib(tqdm(total=2 * len(models), desc=\"GridSearch Stacking\")):\n",
    "        stack.fit(X, y)\n",
    "    joblib.dump(stack,stack_path , compress=3)\n",
    "else:\n",
    "    stack = joblib.load(stack_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions(\n",
    "    df=df_test,model=stack,features=X_cat + X_num,\n",
    "    target=target,path= os.path.join(pred_folder, \"predictions_stack.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea328523",
   "metadata": {},
   "source": [
    "#### Stack Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_stack_cv = True\n",
    "stack_cv_path = os.path.join(cv_folder, \"cv_stack.npy\")\n",
    "\n",
    "if overwrite_stack_cv or not os.path.exists(stack_cv_path):\n",
    "    with tqdm_joblib(tqdm(total=cv.get_n_splits(), desc=\"GridSearch Cross Val Predict\")):\n",
    "        oof_stack = cross_val_predict(stack, X, y,cv=cv, n_jobs=2,method='predict_proba',verbose=10)\n",
    "        np.save(stack_cv_path, oof_stack)\n",
    "else:\n",
    "    oof_stack = np.load(stack_cv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19804e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the results data frames with the stack\n",
    "df_oof[\"stack\"] = oof_stack[:,1]\n",
    "df_oof.to_csv(os.path.join(result_folder, \"cv_oof.csv\"), index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be9529",
   "metadata": {},
   "source": [
    "## 2.3 Compute SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730133fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_folder = os.path.join(result_folder, \"Shap\")\n",
    "os.makedirs(shap_folder, exist_ok=True)\n",
    "\n",
    "overwrite_explainer = False\n",
    "N_BG = 250\n",
    "N_SAMPLE = 1000\n",
    "max_evals = 1024  \n",
    "\n",
    "for model_name in list(models.keys()):\n",
    "    shap_path = os.path.join(shap_folder, f\"shap_{model_name}.joblib\")\n",
    "    if overwrite_explainer or not os.path.exists(shap_path):\n",
    "        print(f\"Running {model_name}\")\n",
    "        sv = compute_shap_payload(models, model_name, X, N_SAMPLE, X_num, X_cat, n_bg=N_BG, \n",
    "                                  max_evals=max_evals) \n",
    "        joblib.dump(sv, shap_path, compress=3)\n",
    "    else:\n",
    "        print(f\"Loading {model_name}\")\n",
    "        sv = joblib.load(shap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144caee0",
   "metadata": {},
   "source": [
    "In this notebook, we established a set of baseline models using only the raw features. This gave us a first benchmark to understand the dataset and evaluate model performance in its simplest form. In the [Next Notebook](3_Bank_Features_Engineering.ipynb), we will move on to feature engineering, where we will transform and enrich the data to give our models a stronger representation and hopefully improve their predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
