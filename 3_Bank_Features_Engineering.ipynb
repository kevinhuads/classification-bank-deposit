{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a66093",
   "metadata": {},
   "source": [
    "# 3. Bank - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b770942",
   "metadata": {},
   "source": [
    "### 3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1f002",
   "metadata": {},
   "source": [
    "So far, we explored the dataset through EDA and trained baseline models using the raw features. While this gave us a first idea of performance, it is unlikely that the original variables in their current form are optimal for modeling. Feature engineering is the step where we refine and transform the data to make it more informative for our algorithms. This can include scaling numerical features, creating interactions, handling categorical variables, or extracting new features that capture hidden patterns through different encoding techniques. The goal is to give the models a richer and cleaner representation of the data, which often leads to better predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf44f05",
   "metadata": {},
   "source": [
    "These transformations are particularly beneficial for gradient boosting algorithms such as LightGBM and XGBoost. Unlike simpler models like logistic regression, which rely heavily on linear relationships, gradient boosting can capture complex interactions between features. Well-engineered features make these interactions clearer and easier to exploit, often leading to significant gains in performance. Compared to bagging methods or neural networks, boosting algorithms also tend to be more optimized to handle large datasets, more sample-efficient and less sensitive to heavy preprocessing than bagging ensembles or neural networks, which makes them a strong choice when working with structured data.\n",
    "\n",
    "(This has been inspired by *Chris Deotte's* [Notebook](https://www.kaggle.com/code/cdeotte/train-more-xgb-nn-lb-0-9774))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910be3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from itertools import combinations\n",
    "from sklearn import set_config\n",
    "import shap\n",
    "from sklearn.base import clone\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_config(transform_output=\"pandas\") \n",
    "\n",
    "from bank_functions import *\n",
    "from bank_feat_engineer import *\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfaa9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"lgb\"\n",
    "\n",
    "result_folder = \"Results\"\n",
    "model_folder = os.path.join(result_folder, model)\n",
    "final_folder = os.path.join(model_folder, \"final_models\")\n",
    "shap_folder = os.path.join(result_folder, \"Shap\")\n",
    "\n",
    "for f in [result_folder, model_folder, final_folder, shap_folder]:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(\"Data/train.csv\")\n",
    "df_test  = pd.read_csv(\"Data/test.csv\")\n",
    "df_orig = pd.read_csv(\"Data/bank-full.csv\", delimiter=\";\")\n",
    "\n",
    "target = \"y\"\n",
    "X_cat = [\"job\",\"month\",\"poutcome\",\"education\",\"contact\",\"marital\",\"loan\",\"housing\",\"default\"]\n",
    "X_num = [\"balance\",\"duration\",\"pdays\",\"age\",\"campaign\",\"previous\",\"day\"]\n",
    "\n",
    "df_test[target] = np.random.randint(0, 2, len(df_test))\n",
    "df_orig[target] = df_orig[target].map({\"yes\": 1, \"no\": 0})\n",
    "df_orig[\"id\"] = (np.arange(len(df_orig)) + 1e6).astype(\"int\")\n",
    "df_orig = df_orig.set_index(\"id\")\n",
    "\n",
    "y = df_train[target]\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577e0f8",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffeb33",
   "metadata": {},
   "source": [
    "There are several techniques we are going to use to enrich the feature space:\n",
    "- *Create categorical twins of the numeric variables*: by binning continuous variables into groups, we allow tree-based models to more easily capture non-linear thresholds and category-like behavior.\n",
    "\n",
    "- *Create pairs of each categorical variable*: combining categories two by two can reveal interaction effects that are not visible when looking at single features in isolation.\n",
    "\n",
    "- *(Global) Count Encoding*: replacing categories with their frequency across the dataset provides an ordinal signal that helps models distinguish between common and rare values.\n",
    "\n",
    "- *(OOF) Feature encoding*: using out-of-fold encodings ensures that target-based transformations are done in a way that avoids leakage, giving the model additional predictive power without compromising validity.\n",
    "\n",
    "Together, these techniques aim to create richer and more expressive features that gradient boosting algorithms can leverage effectively, while keeping training times manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556d980",
   "metadata": {},
   "source": [
    "#### Create categorical twins for each numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8414df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CATS: ['balance2', 'duration2', 'pdays2', 'age2', 'campaign2', 'previous2', 'day2']\n",
      "Cardinality of all CATS: {'balance2': 8590, 'duration2': 1824, 'pdays2': 628, 'age2': 78, 'campaign2': 52, 'previous2': 54, 'day2': 31, 'job': 12, 'month': 12, 'poutcome': 4, 'education': 4, 'contact': 3, 'marital': 3, 'loan': 2, 'housing': 2, 'default': 2}\n"
     ]
    }
   ],
   "source": [
    "# label-encode all categoricals into their original columns\n",
    "# create a \"c2\" categorical twin for each numeric column\n",
    "combine = pd.concat([df_train, df_test, df_orig], axis=0)\n",
    "X_num_as_cat = []\n",
    "cat_card = {}\n",
    "for c in X_num + X_cat:\n",
    "    n = c\n",
    "    if c in X_num:\n",
    "        n = f\"{c}2\"           # twin treated as categorical\n",
    "        X_num_as_cat.append(n)\n",
    "    # factorize into integer codes (NaN -> -1)\n",
    "    codes, uniques = pd.factorize(combine[c], sort=False)\n",
    "    combine[n] = codes.astype(\"int32\")\n",
    "    cat_card[n] = int(combine[n].max()) + 1\n",
    "\n",
    "    # for consistency with the original notebook\n",
    "    # - numeric columns stay numeric int32\n",
    "    # - categorical columns have just been overwritten by their integer codes\n",
    "    combine[c] = combine[c].astype(\"int32\")\n",
    "\n",
    "print(\"New CATS:\", X_num_as_cat)\n",
    "print(\"Cardinality of all CATS:\", cat_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8221b",
   "metadata": {},
   "source": [
    "#### Create Pairwise Categorical Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66d2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 120 new CAT columns\n"
     ]
    }
   ],
   "source": [
    "pairs = combinations(X_cat + X_num_as_cat, 2)\n",
    "new_cols = {}\n",
    "X_pairs = []\n",
    "\n",
    "for c1, c2 in pairs:\n",
    "    name = \"_\".join(sorted((c1, c2)))\n",
    "    new_cols[name] = (combine[c1].astype(np.int64) * int(cat_card[c2])) + combine[c2].astype(np.int64)\n",
    "    X_pairs.append(name)\n",
    "\n",
    "if new_cols:\n",
    "    new_df = pd.DataFrame(new_cols, index=combine.index)\n",
    "    combine = pd.concat([combine, new_df], axis=1)\n",
    "\n",
    "print(f\"Created {len(X_pairs)} new CAT columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48814b03",
   "metadata": {},
   "source": [
    "#### Count Encoding for all categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de99c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 136 columns... 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, \n"
     ]
    }
   ],
   "source": [
    "CE = []\n",
    "CC = X_cat + X_num_as_cat + X_pairs\n",
    "combine[\"i\"] = np.arange(len(combine))  # for restoring order later\n",
    "\n",
    "print(f\"Processing {len(CC)} columns... \", end=\"\")\n",
    "for i, c in enumerate(CC):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}, \", end=\"\")\n",
    "    tmp = combine.groupby(c)[\"y\"].count().astype(\"int32\").rename(f\"CE_{c}\")\n",
    "    CE.append(f\"CE_{c}\")\n",
    "    # merge by key on left column and right index (Series)\n",
    "    combine = combine.merge(tmp, left_on=c, right_index=True, how=\"left\")\n",
    "combine = combine.sort_values(\"i\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9c572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (750000, 282) Test shape (250000, 282) Original shape (45211, 282)\n"
     ]
    }
   ],
   "source": [
    "# Split combine back into train/test/orig (preserve original sizes)\n",
    "N_train, N_test, N_orig = len(df_train), len(df_test), len(df_orig)\n",
    "df_train = combine.iloc[:N_train].copy()\n",
    "df_test  = combine.iloc[N_train:N_train + N_test].copy()\n",
    "df_orig  = combine.iloc[N_train + N_test:].copy()\n",
    "del combine\n",
    "print(\"Train shape\", df_train.shape, \"Test shape\", df_test.shape, \"Original shape\", df_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d5083",
   "metadata": {},
   "source": [
    "### 3.3 Data and Feature Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56645a3e",
   "metadata": {},
   "source": [
    "On top of that, we will also leverage the original dataset **orig** as an additional source of information, applying two augmentation strategies separately in order to avoid leakage:\n",
    "- *Data Augmentation*: we expand the training set by concatenating the rows from **orig**, increasing sample size and diversity.\n",
    "- *Feature Augmentation*: we enrich the dataset with new features computed through target encoding on **orig**, providing extra predictive signals.\n",
    "To measure the impact of these approaches, we will keep the base dataset without augmentation as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581e80b",
   "metadata": {},
   "source": [
    "#### Target Encoding with Original Data as Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efae968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 136 columns... 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, \n"
     ]
    }
   ],
   "source": [
    "# self-contained KFold target encoder (smooth=0 equivalent)\n",
    "\n",
    "TE = []\n",
    "print(f\"Processing {len(CC)} columns... \", end=\"\")\n",
    "for i, c in enumerate(CC):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}, \", end=\"\")\n",
    "    tmp = df_orig.groupby(c)[\"y\"].mean().astype(\"float32\").rename(f\"TE_ORIG_{c}\")\n",
    "    NAME = f\"TE_ORIG_{c}\"\n",
    "    TE.append(NAME)\n",
    "    # merge series by index\n",
    "    df_train = df_train.merge(tmp, left_on=c, right_index=True, how=\"left\")\n",
    "    df_test  = df_test.merge(tmp, left_on=c, right_index=True, how=\"left\")\n",
    "# sorting back to original order\n",
    "df_train = df_train.sort_values(\"i\")\n",
    "df_test  = df_test.sort_values(\"i\")\n",
    "ORIG_TE_COLS = [c for c in df_train.columns if c.startswith(\"TE_ORIG_\")]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeeea6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Results\\\\lgb\\\\features.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the features\n",
    "CAT_CANDIDATES = set(X_cat) | set(X_num_as_cat) | set(X_pairs)\n",
    "features = {\n",
    "    \"X_cat\": X_cat,\n",
    "    \"X_num\": X_num,\n",
    "    \"X_num_as_cat\": X_num_as_cat,\n",
    "    \"X_pairs\": X_pairs,\n",
    "    \"CE\": CE,\n",
    "    \"ORIG_TE_COLS\": ORIG_TE_COLS,\n",
    "    \"CAT_CANDIDATES\": CAT_CANDIDATES,\n",
    "}\n",
    "joblib.dump(features, os.path.join(model_folder, \"features.joblib\"), compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbee28e",
   "metadata": {},
   "source": [
    "### 3.4 Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13501c47",
   "metadata": {},
   "source": [
    "For each feature setup, we will train a LightGBM model using *N_Folds* cross-validation and produce two types of outputs:\n",
    "- **OOF predictions**, used to evaluate the performance of each setup on the training data.\n",
    "- **Test set predictions**, averaged across folds and submitted to Kaggle, allowing us to compare the different models directly on the competition leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2fb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSES = [\n",
    "    (\"vanilla_\", False, None, 0),     # Pass 1: vanilla runs for benchmark\n",
    "    (\"origrow_\", False, df_orig, 1),  # Pass 2: Data augmentation\n",
    "    (\"origcol_\", True,  None, 0),     # Pass 3: Feature augmentation (TE_ORIG_*)\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Define and run the feature sets\n",
    "# ===============================\n",
    "BASE_SETUPS = [\n",
    "    (\"base\",                      X_cat + X_num),\n",
    "    (\"with_num_as_cat\",           X_cat + X_num + X_num_as_cat),\n",
    "    (\"with_pairs\",                X_cat + X_num + X_pairs),\n",
    "    (\"with_ce\",                   X_cat + X_num + CE),\n",
    "    (\"with_num_as_cat_and_pairs\", X_cat + X_num + X_num_as_cat + X_pairs),\n",
    "    (\"with_num_as_cat_and_ce\",    X_cat + X_num + X_num_as_cat + CE),\n",
    "    (\"with_pairs_and_ce\",         X_cat + X_num + X_pairs + CE),\n",
    "    (\"with_all\",                  X_cat + X_num + X_num_as_cat + X_pairs + CE),\n",
    "]\n",
    "\n",
    "# Expand each into the (no_te, te) variants\n",
    "SETUPS = [\n",
    "    (name, features, False)\n",
    "    for name, features in BASE_SETUPS\n",
    "] + [\n",
    "    (name + \"_te\", features, True)\n",
    "    for name, features in BASE_SETUPS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df77ca40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vanilla_base] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_num_as_cat] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_pairs] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_num_as_cat_and_pairs] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_num_as_cat_and_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_pairs_and_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_all] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_base_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_num_as_cat_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_pairs_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_num_as_cat_and_pairs_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_num_as_cat_and_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_pairs_and_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[vanilla_with_all_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_base] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_num_as_cat] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_pairs] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_num_as_cat_and_pairs] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_num_as_cat_and_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_pairs_and_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_all] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_base_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_num_as_cat_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_pairs_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_num_as_cat_and_pairs_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_num_as_cat_and_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_pairs_and_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origrow_with_all_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_base] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_num_as_cat] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_pairs] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_num_as_cat_and_pairs] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_num_as_cat_and_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_pairs_and_ce] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_all] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_base_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_num_as_cat_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_pairs_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_num_as_cat_and_pairs_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_num_as_cat_and_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_pairs_and_ce_te] Models, OOF and predictions all exist. Skipping entirely.\n",
      "[origcol_with_all_te] Models, OOF and predictions all exist. Skipping entirely.\n"
     ]
    }
   ],
   "source": [
    "for prefix, add_orig_cols, train_more_orig, append_orig_times in PASSES:\n",
    "    for set_name, feature_list, use_te in SETUPS:\n",
    "        base_feats = feature_list\n",
    "        if add_orig_cols:\n",
    "            base_feats = list(dict.fromkeys(feature_list + ORIG_TE_COLS))\n",
    "        run_feature_set(\n",
    "            train_df=df_train,\n",
    "            test_df=df_test,\n",
    "            y=y,\n",
    "            base_features=base_feats,\n",
    "            cat_candidates=CAT_CANDIDATES,\n",
    "            set_name=f\"{prefix}{set_name}\",\n",
    "            use_target_encoding=use_te,\n",
    "            save_folder=os.path.join(result_folder,model),\n",
    "            model=model,\n",
    "            te_inner_splits=5,\n",
    "            seed=SEED,\n",
    "            n_folds=N_FOLDS,\n",
    "            train_more_orig=train_more_orig,\n",
    "            append_orig_times=append_orig_times,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b98b3",
   "metadata": {},
   "source": [
    "The next step is to run the two full models: one with row augmentation and the other with column augmentation, using the complete set of engineered features. This will allow us to directly compare how each augmentation strategy performs when the feature space is maximized.\n",
    "\n",
    "For both models, we are also going to compute the SHAP values in order to analyze feature importance. This will help us understand which engineered features and augmentation strategies contribute the most to the predictions, and whether the gains in performance come from specific transformations or from the combination of all features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f32db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1560]\tvalid_0's binary_logloss: 0.0140083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Results\\\\lgb\\\\final_models\\\\origrow_with_all_te.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_aug = \"origrow_with_all_te\"\n",
    "col_aug = \"origcol_with_all_te\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1) origrow_with_all_te\n",
    "#    - augment rows with df_orig\n",
    "#    - add TE on the augmented matrix\n",
    "# ---------------------------\n",
    "with_all = X_cat + X_num + X_num_as_cat + X_pairs + CE\n",
    "X_tr_base = df_train[with_all].reset_index(drop=True)\n",
    "y_tr_base = y.reset_index(drop=True)\n",
    "\n",
    "X_aug = pd.concat([X_tr_base, df_orig[with_all].reset_index(drop=True)], axis=0, ignore_index=True)\n",
    "y_aug = pd.concat([y_tr_base, df_orig[\"y\"].reset_index(drop=True)], axis=0, ignore_index=True)\n",
    "\n",
    "X_aug_te, used_feats_row = build_full_te_matrix(\n",
    "    df_X=X_aug,\n",
    "    y_vec=y_aug,\n",
    "    base_features=with_all,\n",
    "    cat_candidates=CAT_CANDIDATES\n",
    ")\n",
    "\n",
    "clf_row, used_feats, best_iter = fit_with_holdout(X_aug_te, y_aug, used_features = used_feats_row, \n",
    "                                                  model=\"lgb\", n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "# Matrix for SHAP explanations (training portion only for interpretability)\n",
    "X_explain_row = X_aug_te.iloc[:len(df_train)]\n",
    "\n",
    "joblib.dump(clf_row,os.path.join(final_folder, f\"{row_aug}.joblib\"),compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757d5628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1277]\tvalid_0's binary_logloss: 0.0132933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Results\\\\lgb\\\\final_models\\\\origcol_with_all_te.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2) origcol_with_all_te\n",
    "#    - append ORIG_TE_COLS, then add TE on train\n",
    "# ---------------------------\n",
    "with_all_and_origcol = list(dict.fromkeys(with_all + ORIG_TE_COLS))\n",
    "X_col = df_train[with_all_and_origcol].reset_index(drop=True)\n",
    "y_col = y.reset_index(drop=True)\n",
    "\n",
    "X_col_te, used_feats_col = build_full_te_matrix(\n",
    "    df_X=X_col,\n",
    "    y_vec=y_col,\n",
    "    base_features=with_all_and_origcol,\n",
    "    cat_candidates=CAT_CANDIDATES\n",
    ")\n",
    "\n",
    "clf_col, used_feats, best_iter = fit_with_holdout(X_col_te, y_col, used_features=used_feats_col, \n",
    "                                                  model=\"lgb\", n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "joblib.dump(clf_col,os.path.join(final_folder, f\"{col_aug}.joblib\"),compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "405f8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP ---------------------------\n",
    "pairs = [\n",
    "    (row_aug, clf_row, X_explain_row),\n",
    "    (col_aug, clf_col, X_col_te),\n",
    "]\n",
    "N_SAMPLE = 25000\n",
    "for name, clf, X in pairs:\n",
    "    path = os.path.join(shap_folder, f\"shap_{name}.joblib\")\n",
    "    expl = shap.TreeExplainer(clf)\n",
    "    sample = X.sample(n=min(len(X), N_SAMPLE), random_state=SEED)\n",
    "    sv = expl(sample, check_additivity=False)\n",
    "    joblib.dump(sv, path, compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a8d4a",
   "metadata": {},
   "source": [
    "## 3.5 Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7eefb4",
   "metadata": {},
   "source": [
    "Our models are ready so we can generate both the CV OOF for the post-model analysis and the predictions for the Kaggle submissions. We will generate two setups :\n",
    "- A *blend* which is basically the aggregated results of the passes with **every added features** with *Data (rows) Augments* and *Features (columns) Augments*\n",
    "- A final *stacking model* which will take all the models that we computed both here and in the [Previous Notebook](2_Bank_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29409900",
   "metadata": {},
   "source": [
    "#### Cross Validation Out Of Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cdae650",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_oof = pd.read_csv(os.path.join(result_folder, \"cv_oof.csv\"))\n",
    "augments = [\"row\",\"col\"]\n",
    "df_fe = pd.DataFrame({\n",
    "    f\"{model}_{aug}_augments\": pd.read_csv(f).squeeze(\"columns\")\n",
    "    for aug in augments\n",
    "    for f in [os.path.join(result_folder, model, \"oof\", f\"orig{aug}_with_all_te_oof.csv\")]\n",
    "})\n",
    "\n",
    "# Write the OOF file for the blend model\n",
    "blend_oof = df_fe.mean(axis = 1)\n",
    "pd.DataFrame({\"oof\":blend_oof}).to_csv(os.path.join(final_folder,\"fe_blend_oof.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "764e6c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[663]\tvalid_0's binary_logloss: 0.125121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Results\\\\lgb\\\\final_models\\\\fe_stack.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = pd.read_csv(os.path.join(result_folder, \"cv_oof.csv\")).iloc[:, :-1]\n",
    "df_merged_oof = pd.concat([df_base,df_fe], axis=1)\n",
    "\n",
    "# Train final meta-learner on all data\n",
    "final_meta, all_features, best_iter = fit_with_holdout(df_merged_oof, y)\n",
    "\n",
    "# Write the OOF file for the final stack model\n",
    "cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "est_for_cv = clone(final_meta).set_params(n_estimators = best_iter)\n",
    "final_oof = cross_val_predict(est_for_cv, df_merged_oof, y, cv=cv, n_jobs=8, method=\"predict_proba\")[:, 1]\n",
    "\n",
    "pd.DataFrame({\"oof\":final_oof}).to_csv(os.path.join(final_folder, \"fe_stack_oof.csv\"), index=False)\n",
    "joblib.dump(final_meta,os.path.join(final_folder, \"fe_stack.joblib\"),compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54690a01",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdc37abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_te_pred = pd.DataFrame({\n",
    "    f\"lgb_{aug}_augments\": pd.read_csv(f)[\"y\"]\n",
    "    for aug in augments\n",
    "    for f in [os.path.join(result_folder, model, \"predictions\", f\"predictions_orig{aug}_with_all_te.csv\")]\n",
    "})\n",
    "base_pred = pd.DataFrame({\n",
    "    os.path.splitext(os.path.basename(f))[0][len(\"predictions_\"):]: pd.read_csv(f).squeeze(\"columns\").iloc[:,1]\n",
    "    for f in sorted(glob(os.path.join(result_folder, \"predictions\", \"*\"))) \n",
    "})\n",
    "\n",
    "base_pred = pd.concat([base_pred,all_te_pred], axis = 1)\n",
    "base_pred = base_pred[df_merged_oof.columns]\n",
    "\n",
    "final_pred = pd.DataFrame({\"id\":df_test[\"id\"],\n",
    "                           \"y\":final_meta.predict_proba(base_pred)[:,1]})\n",
    "final_pred.to_csv(os.path.join(final_folder, \"predictions_fe_stack.csv\"), index=False)\n",
    "\n",
    "df_blend = final_pred \n",
    "df_blend[target] = all_te_pred.mean(axis = 1)\n",
    "df_blend.to_csv(os.path.join(final_folder, \"predictions_fe_blend.csv\"),index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b09162",
   "metadata": {},
   "source": [
    "Now that our models are ready, we are going to analyze them in the [Final Analysis Notebook](4_Bank_Final_Analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
